{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WIT COMPAS",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiNxsd4_q9wq",
        "colab_type": "text"
      },
      "source": [
        "### What-If Tool on COMPAS\n",
        "\n",
        "Copyright 2019 Google LLC.\n",
        "SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "This notebook shows use of the [What-If Tool](https://pair-code.github.io/what-if-tool) on the COMPAS dataset.\n",
        "\n",
        "\n",
        "For ML fairness background on COMPAS see:\n",
        "- https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
        "- https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm\n",
        "- http://www.crj.org/assets/2017/07/9_Machine_bias_rejoinder.pdf\n",
        "\n",
        "The dataset is from the [COMPAS kaggle page](https://www.kaggle.com/danofer/compass).\n",
        "\n",
        "This notebook trains a linear classifier on the on the COMPAS dataset to mimic the behavior of the the COMPAS recidivism classifier. We can then analyze our COMPAS proxy model for fairness using the What-If Tool.\n",
        "\n",
        "The specific binary classification task for this model is to determine if a person belongs in the \"Low\" risk class according to COMPAS (negative class), or the \"Medium\" or \"High\" risk class (positive class)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqB2tjOMETmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Install the What-If Tool widget if running in colab {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  !pip install --upgrade witwidget\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlwjF-Nnmoww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Define helper functions {display-mode: \"form\"}\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import functools\n",
        "\n",
        "# Creates a tf feature spec from the dataframe and columns specified.\n",
        "def create_feature_spec(df, columns=None):\n",
        "    feature_spec = {}\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for f in columns:\n",
        "        if df[f].dtype is np.dtype(np.int64):\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
        "        elif df[f].dtype is np.dtype(np.float64):\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.float32)\n",
        "        else:\n",
        "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.string)\n",
        "    return feature_spec\n",
        "\n",
        "# Creates simple numeric and categorical feature columns from a feature spec and a\n",
        "# list of columns from that spec to use.\n",
        "#\n",
        "# NOTE: Models might perform better with some feature engineering such as bucketed\n",
        "# numeric columns and hash-bucket/embedding columns for categorical features.\n",
        "def create_feature_columns(columns, feature_spec):\n",
        "    ret = []\n",
        "    for col in columns:\n",
        "        if feature_spec[col].dtype is tf.int64 or feature_spec[col].dtype is tf.float32:\n",
        "            ret.append(tf.feature_column.numeric_column(col))\n",
        "        else:\n",
        "            ret.append(tf.feature_column.indicator_column(\n",
        "                tf.feature_column.categorical_column_with_vocabulary_list(col, list(df[col].unique()))))\n",
        "    return ret\n",
        "\n",
        "# An input function for providing input to a model from tf.Examples\n",
        "def tfexamples_input_fn(examples, feature_spec, label, mode=tf.estimator.ModeKeys.EVAL,\n",
        "                       num_epochs=None, \n",
        "                       batch_size=64):\n",
        "    def ex_generator():\n",
        "        for i in range(len(examples)):\n",
        "            yield examples[i].SerializeToString()\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "      ex_generator, tf.dtypes.string, tf.TensorShape([]))\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example, label, feature_spec))\n",
        "    dataset = dataset.repeat(num_epochs)\n",
        "    return dataset\n",
        "\n",
        "# Parses Tf.Example protos into features for the input function.\n",
        "def parse_tf_example(example_proto, label, feature_spec):\n",
        "    parsed_features = tf.io.parse_example(serialized=example_proto, features=feature_spec)\n",
        "    target = parsed_features.pop(label)\n",
        "    return parsed_features, target\n",
        "\n",
        "# Converts a dataframe into a list of tf.Example protos.\n",
        "def df_to_examples(df, columns=None):\n",
        "    examples = []\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for index, row in df.iterrows():\n",
        "        example = tf.train.Example()\n",
        "        for col in columns:\n",
        "            if df[col].dtype is np.dtype(np.int64):\n",
        "                example.features.feature[col].int64_list.value.append(int(row[col]))\n",
        "            elif df[col].dtype is np.dtype(np.float64):\n",
        "                example.features.feature[col].float_list.value.append(row[col])\n",
        "            elif row[col] == row[col]:\n",
        "                example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
        "        examples.append(example)\n",
        "    return examples\n",
        "\n",
        "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
        "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
        "def make_label_column_numeric(df, label_column, test):\n",
        "  df[label_column] = np.where(test(df[label_column]), 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu398ARdeuxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Read training dataset from CSV {display-mode: \"form\"}\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://storage.googleapis.com/what-if-tool-resources/computefest2019/cox-violent-parsed_filt.csv')\n",
        "df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67DYIFxoevt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Specify input columns and column to predict {display-mode: \"form\"}\n",
        "import numpy as np\n",
        "\n",
        "# Filter out entries with no indication of recidivism or no compass score\n",
        "df = df[df['is_recid'] != -1]\n",
        "df = df[df['decile_score'] != -1]\n",
        "\n",
        "# Rename recidivism column\n",
        "df['recidivism_within_2_years'] = df['is_recid']\n",
        "\n",
        "# Make the COMPASS label column numeric (0 and 1), for use in our model\n",
        "df['COMPASS_determination'] = np.where(df['score_text'] == 'Low', 0, 1)\n",
        "\n",
        "# Set column to predict\n",
        "label_column = 'COMPASS_determination'\n",
        "\n",
        "# Get list of all columns from the dataset we will use for model input or output.\n",
        "input_features = ['sex', 'age', 'race', 'priors_count', 'juv_fel_count', 'juv_misd_count', 'juv_other_count']\n",
        "features_and_labels = input_features + [label_column]\n",
        "\n",
        "features_for_file = input_features + ['recidivism_within_2_years', 'COMPASS_determination']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV4f_4_Lex22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Convert dataset to tf.Example protos {display-mode: \"form\"}\n",
        "\n",
        "examples = df_to_examples(df, features_for_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyLr-_0de1Ii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Create and train the classifier {display-mode: \"form\"}\n",
        "\n",
        "num_steps = 2000  #@param {type: \"number\"}\n",
        "\n",
        "# Create a feature spec for the classifier\n",
        "feature_spec = create_feature_spec(df, features_and_labels)\n",
        "\n",
        "# Define and train the classifier\n",
        "train_inpf = functools.partial(tfexamples_input_fn, examples, feature_spec, label_column)\n",
        "classifier = tf.estimator.LinearClassifier(\n",
        "    feature_columns=create_feature_columns(input_features, feature_spec))\n",
        "classifier.train(train_inpf, steps=num_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ-rK11X5arK",
        "colab_type": "text"
      },
      "source": [
        "### What-If Tool analysis\n",
        "\n",
        "We can see the same unfairness that ProPublica found in their analysis by:\n",
        "1. Going the the \"Performance + Fairness\" tab\n",
        "2. Setting \"Ground Truth Feature\" to \"recidivism_within_2_years\"\n",
        "3. In \"Slice by\" dropdown menu, select \"race\"\n",
        "4. Look at the confusion matrices of the \"African-American\" and \"Causasian\" slices.\n",
        "  - They have very similar accuracy (TP+TN)/Total \n",
        "  - But, the FP rate is MUCH higher for African Americans and the FN rate is MUCH higher for caucasians"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUQVro76e38Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Invoke What-If Tool for test data and the trained models {display-mode: \"form\"}\n",
        "\n",
        "\n",
        "num_datapoints = 10000  #@param {type: \"number\"}\n",
        "tool_height_in_px = 1000  #@param {type: \"number\"}\n",
        "\n",
        "from witwidget.notebook.visualization import WitConfigBuilder\n",
        "from witwidget.notebook.visualization import WitWidget\n",
        "\n",
        "# Setup the tool with the test examples and the trained classifier\n",
        "config_builder = WitConfigBuilder(examples[0:num_datapoints]).set_estimator_and_feature_spec(\n",
        "    classifier, feature_spec)\n",
        "WitWidget(config_builder, height=tool_height_in_px)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOVamCz1LsTd",
        "colab_type": "text"
      },
      "source": [
        "#### Exploration ideas\n",
        "\n",
        "- Organize datapoints by \"inference score\" (can do this through binning or use of scatter plot) to see points ordered by how likely they were determined to re-offend.\n",
        "  - Select a point near the boundary line (where red points turn to blue points)\n",
        "  - Find the nearest counterfactual to see a similar person with a different decision. What is different?\n",
        "  - Look at the partial dependence plots for the selected person. What changes in what features would change the decision on this person?\n",
        "- In \"Performance and Fairness\" tab, slice the dataset by different features (such as race or sex)\n",
        "  - Look at the confusion matrices for each slice - How does performance compare in those slices? What from the training data may have caused the difference in performance between the slices? What root causes could exist?\n",
        "  - Use the threshold optimization buttons to optimize positive classification thresholds for each slice based on any of the possible fairness constraints - How different do the thresholds have to be to achieve that constraint? How varied are the thresholds depending on the fairness constraint chosen?\n",
        "\n",
        "- In the \"Performance + Fairness\" tab, change the cost ratio so that you can optimize the threshold based off of a non-symmetric cost of false positives vs false negatives. Then click the \"optimize threshold\" button and see the effect on the confusion matrix. \n",
        "  - Slice the dataset by a feature, such as sex or race. How has the new cost ratio affected the disparity in performance between slices? Click the different threshold optimization buttons to see how the changed cost ratio affects the disparity given different fairness constraints.\n",
        "\n",
        "- Try adding/removing features from the set of input features that the model uses during training. The model trained by this notebook only uses 7 of the columns from the dataset, as defined in the \"Specify input columns and column to predict\" cell in this notebook. How does your new set of input features affect the model performance (overall and across slices).\n",
        "\n",
        "- If you set the ground truth feature to \"COMPAS_determination\" in the \"Performance + Fairness\" tab, you will see the confusion matrix and ROC curve of how good our model is at being a proxy for the COMPAS model itself (as opposed to how good our model is at predicting recidivism).\n",
        "  - How well is our model doing at its task? What types of errors does it have?\n",
        "  - Try improving the performance of the model. Options include adding more input features, changing the model architecture, and training for more steps.\n",
        "  - After you've improved our proxy COMPAS model, what (if any) change in unfairness do you see when evaluating against \"recidivism_after_2_years\"?\n",
        "\n"
      ]
    }
  ]
}
